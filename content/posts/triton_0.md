---
title: "Triton Tutorial 0: Introduction"
date: 2023-08-01
draft: false
---

This is part 1 of a tutorial series. Find the other parts here:

- [Part 0: Introduction](https://lweitkamp.github.io/posts/triton_0/)
- [Part 1: Vector Addition](https://lweitkamp.github.io/posts/triton_1/)


# Introduction
I wanted to learn Triton but the documentation is a bit sparse and the examples are just that - examples of code.
So I decided to change some of the examples into homework-like assignments to be completed, and added some more exercises to it.
The exercises lead up to an implementation of flash attention in Triton that is wrapped in a PyTorch layer.

# What is Triton

# Reading List
The website has some good getting-started documentation:
- [Triton Installation Guide](https://triton-lang.org/main/getting-started/installation.html)
- [Triton Introduction](https://triton-lang.org/main/programming-guide/chapter-1/introduction.html)
- [Triton Related Work](https://triton-lang.org/main/programming-guide/chapter-2/related-work.html)

Further reading:
- [Philippe Tillet's PhD](https://dash.harvard.edu/handle/1/37368966)
- [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf)
