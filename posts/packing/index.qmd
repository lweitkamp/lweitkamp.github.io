---
title: "Packing"
author: "Laurens Weitkamp"
date: "2023-05-01"
draft: false
categories: [transformer, trainign, packing, language modelling]

format:
  html:
    toc: true
    toc-location: left
reference-location: margin
citation-location: margin
---

The context length of transformer models is rapidly increasing with each new generation of frontier models. Google's latest Gemini 1.5 can hold up to 1 million tokens, which amounts to 1 hour of video or 700,000 words. This is driven by innovations in hardware and smarter algorithms tailored to the specific demands of long-context training[^1].

[^1]: There is an orthogonal direction here in which we train with a short context, but inference on long context. Packing can still apply during inference.

Long context brings several challenges to the mix: we need to figure out data that is inherently long context (books, podcasts, videos) but we also need to ensure we treat smaller context data efficiently. If we simply concatenate these documents and throw them in an autoregressive transformer, we are *cross-contamination* sequences: predicting a sequence `S2` from an unrelated sequence `S1` can be almost impossible given a large difference in subject matter.

The same holds for inference. If we are dynamically batching tokens from different users, we could have an efficiency gain if we put shorter conversations in the same 'batch'. The solution to both issues is a technique called *packing*.

# Packing

As an example, take a transformer with context length `ctx` and two samples `S1` and `S2` of lengths `n`, `m` respectively with `n + m <= ctx`. If we take these as separate sequences we are left with padding `ctx - n + ctx - m` tokens. If we pack them together, we reduce the padding to only `ctx - n - m`! The figure below depicts two sequences and how we pack them.

![Two sequences "packed".](packed_sequences.png)


## Literature Review
Packing is briefly described in most papers on language modelling (in fact, most authors cite T5 for it), here are some from the literature:

| Paper | Quote |
| -- | -- |
| RoBERTa[^2] | *"Each input is ***packed*** with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens."* |
| GPT-3[^3] | *"During training we always train on sequences of the full nctx = 2048 token context window, ***packing*** multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency."* |
| T5[^4] | *"Whenever possible, we “***pack***” multiple sequences into each entry of the batch so that our batches contain roughly 216 = 65,536 tokens."* |
| T0[^5] | *"we use ***packing*** to combine multiple training examples into a single sequence to reach the maximum sequence length."* |

[^2]: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
[^3]: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
[^4]: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
[^5]: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)

Some approaches use the packing described above naively and add an `end-of-document` token after the sequence is done to let the model figure out the difference between samples. This *does* seem to hurt performance due to cross-contamination mentioned before, but it might not have as big of an impact at scale[^5].

[^5]: [Efficient Sequence Packing without Cross-contamination](https://arxiv.org/abs/2107.02027)

## Dealing with Cross Contamination

If you want to ensure no cross-contamination is possible, we will need to update the positional information and the attention masks in addition to simply concatenating the sequences.

### Update Positional Information

If we are using some absolute positional encodings (or ALiBi) we will need to 'reset' the positional encoding index starting at each new sample in the pack:

![Packed positional encodings.](packed_positional_encoding.png)

Similar solutions are possible for RoPE (reset the rotation angles) and absolute positional encodings (reset the starting index).

### Update the Attention Mask
Masking in self-attention needs to ensure that one sequence cannot attend to another sequence.
We need to *merge* autoregressive masks for the self-attention layer to ensure this restriction:

![A packed attention mask.](packed_attention_mask.png)

This might be the most intrusive part to the codebase - it will require actually calculating a distinct attention mask per batch where we otherwise can simply have the standard lower diagonal created once for the whole batch. I imagine implementing this efficiently for ALiBi is not an enjoyable assignment.

### That's it!
If properly implemented, you might notice that it makes no difference if we have `N` sequences or `L < N` packed sequences sourced from the `N` - the gradient step should be equal. It's worth your time looking at figures [3](https://arxiv.org/pdf/2107.02027.pdf#page=7) and [4](https://arxiv.org/pdf/2107.02027.pdf#page=8), it's from a paper that introduces the concept of cross-contamination. The paper additionally explains the performance of packing and the effect of 'proper' masking, it's a great read!

Another paper that discusses masked packing but during inference time that goes in depth on the effect of packing on prefilling and the time to first token is [Prepacking](https://arxiv.org/abs/2404.09529).

## Can we Ignore Cross Contamination?
From personal experience, there is little support in public codebases for packing, and this is often justified by mentioning the data is probably not *too* correlated to begin with. In fact, in [Structured Packing in LLM Training Improves Long Context Utilization](https://arxiv.org/abs/2312.17296), the approach essentially ignores the proper masking to **benefit** long context learning!

It is interesting to see that GPT-3 did not use masked packing (from the quote above). It is quite possible that these cross contamination performance issues are not present in large scale training. Ofcourse, GPT-4 might be trained by masked packing, but we will never know.

Does Gemini/PaLM use masked packing? If they use t5x it seems so[^8]:

[^8]: They call it *segmented data* - [from the T5x codebase](https://github.com/google-research/t5x/blob/df5da64315dd8ee269626f66bf60eb8f12a37124/t5x/examples/t5/network.py#L310-L317).

> A sequence length of 2048 was used for all models. Input examples are concatenated together and then split into sequences of exactly 2048 tokens, so that there are no padding tokens, but examples may be split in the middle. Input examples are differentiated from one another with a special [eod] token.
