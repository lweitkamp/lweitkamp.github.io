---
title: "Tensor Parallelism"
author: "Laurens Weitkamp"
date: "2024-01-25"
draft: false
categories: [numpy, mpi]

format:
  html:
    toc: false
    toc-expand: true
    toc-location: left
reference-location: margin
citation-location: margin
---

# Introduction
Tensor parallel, introduced in the Megatron-LM paper by NVidia as *intra-layer model-parallelism*, is a technique for sharding model parameters across GPUs to reduce memory costs. It's typically used in training large models where a single layer cannot fit into a GPU by itself.

This post explores tensor parallelism, based on my experience implementing it in NuMPItron, a small library for distributed transformer training using NumPy and MPI[^1].
Many existing blog posts cover the basics of tensor parallelism—such as splitting linear layers on rows or columns and dividing attention heads—this post will focus on:

[^1]: [NuMPItron](https://github.com/lweitkamp/numpitron). Since matmuls in CPUs are (very quickly) compute-bound, tensor parallelism will actually speed training up by quite a bit.

- The embedding table and the loss function.

- Changes involved when using the 'shazeer' architecture.

- How tensor parallel integrates with other forms of parallel training.

Tensor parallelism, like pipeline model parallelism (inter-layer), requires invasive code changes[^2] and is as slow as the slowest device communication. Hence, it is recommended to use tensor parallelism within a single node with high-speed communication like NVLink. Typically, $N_\text{tp} = 8$, splitting the model weights over 8 devices, is the default for *large* runs. 

[^2]: there are several frameworks supporting it. Megatron-LM is the OG. Since 2023, Microsoft DeepSpeed has support for tensor parallel models too and since 2024 Nanotron by Hugginface also supports tensor parallel training.


# Tensor Parallel Transformer

We begin with a batch of tokens `BROADCAST` to each device.

## Input Embedding
The embedding layer takes a token index, locates the corresponding row in the embedding table, and returns that $d_\text{model}$ dimensional row.
In a tensor parallel model, the rows are divided into chunks across devices.

An issue arises when a device encounters a token index outside its chunk, which can be solved by masking.
Each device returns a tensor of size (batch size, sequence length, $d_\text{model}$), filling out-of-bounds tokens for its chunk with zeros.
Once each device completes this step, an `ALLREDUCE` operation[^3] ensures that masked indices are properly filled across all devices.

[^3]: `ALLREDUCE` counter forward pass: $1 \times N_\text{batch} \times N_\text{ctx} \times d_\text{model}$.

The figure below visualizes this process. If you'd rather see it implemented, refer to the [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/nn/embedding.py#L44-L67) codebase.

![](embedding_tensor_parallel.png)

Are there any alternatives to this? Why wouldn't we slice the tensor on the $d_\text{model}$ dim instead? I think this would be straightforward - each device would create a full batch size by sequence length by $\frac{d_\text{model}}{N_\text{tp}}$ tensor. It wouldn't require masking, but it *would* require an `ALLGATHER`, which has a worse worse complexity vs `ALLREDUCE`.


## Attention
The attention layer transforms the inputs into Query, Key, and Value tensors with weights $W_Q, W_K, W_V,$ each of shape $d_\text{model} \times n_\text{heads} \times d_\text{head}$. Each head calculates the attention weights and the attention values separately, and projects it back to a shared representation with weight matrix $W_O$ of shape $d_\text{head} \times n_\text{heads} \times d_\text{model}$.

Because values are calculated separately, it is simple to split computation along each head, which is what tensor parallel does. Each GPU will therefore have in total $4 \times d_\text{model} \times \frac{N_\text{heads}}{2} \times d_\text{head}$ weights in memory (we assume no biases for the attention layer). An `ALLREDUCE`[^7] collects the output per device. 

[^7]: . `ALLREDUCE` counter forward pass: $\large(N_\text{batch} \times N_\text{ctx} \times d_\text{model}\large) \large(1 + N_\text{layers}\large)$, backward pass: $N_\text{layers} \times N_\text{batch} \times N_\text{ctx} \times d_\text{model}$


### Grouped/Multi-Query Attention
Multi-query attention (MQA) is a modification to the transformer model where the key and value head size is reduced from $N_\text{heads}$ to $1$ to vastly reduce memory consumption when decoding. Grouped query attention (GQA) was introduced more recently as an interpolation between vanilla and multi-query attention to ensure quality does not degrade too much by the reduction in total parameters. 

We have a bit of an issue when using MQA with $TP=8$, since we can't divide the single head to each GPU. A practical solution to this is to replicate the head to $TP$ size effectively using the same head on each device. In general you will see that GQA is used more often and it is set to 8 specifically to serve 8 GPUs in parallel[^8].

[^8]: Trainium GPUs have similar issue since they come in 32 per node, they also advise to replicate the heads: [https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/api_guide.html#gqa-qkv-linear-module](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/api_guide.html#gqa-qkv-linear-module).


## Feed Forward Network
The feed forward net is a simple two layer multi layer perceptron with a ReLU nonlinearity in the middle:
$$
\large \text{FFN}_\text{MLP} = \max(xW_1 + b_1, 0)W_2 + b_2
$$
This means we have two weight matrices including biases that we need to shard across devices in some way that makes mathematical sense.

Depicted below is a 2D matrix multiplication, something like $xW$. We can shard computation along its columns ("column-parallel") or along its rows ("row-parallel").

![](2d_matmul.png)

If we do a column-parallel sharding strategy, we end up with complete sums but the results are sharded across GPUs, requiring an `ALLGATHER` operation.

![](column_parallel.png)

Looking at the row-parallel strategy instead, we end up with partial sums across GPUs that require an `ALLREDUCE`.

![](row_parallel.png)

Following the matrix multiplication with a ReLU or any nonlinearity, we can see that row-parallel will have some issues here, $\text{ReLU}(-5) + \text{ReLU}(14) \neq \text{ReLU}(9)$. Performing a column-parallel strategy **first** ensures we can perform any nonlinearity since the values are complete already. We can follow it with a row-parallel matrix multiplication and `ALLREDUCE`[^10] the results for minimal communication overhead. An additional `AllREDUCE` is required at the end of the backward pass too.

[^10]: `ALLREDUCE` counter forward pass: $\large( N_\text{batch} \times N_\text{ctx} \times d_\text{model}\large) \large(1 + 2 \times N_\text{layers}\large)$, backward pass: $2 \times N_\text{layers} \times N_\text{batch} \times N_\text{ctx} \times d_\text{model}$

For the bias terms, we need to ensure that the column-parallel multiplication adds the bias **only on a single device**, and for the row-parallel multiplication we can split the bias.

### SwiGLU
Swish Gated Linear Units (SwiGLU) combine the Swish activation function[^11] with a Gated Linear Unit: a component-wise product of two linear layers. The result is that we have *three* weight matrices instead of two, and we omit any bias:

[^11]: . $\text{Swish}_\beta(x) = x \sigma (\beta x)$ where $\sigma(x)$ is the sigmoid activation function.

$$
\large \text{FFN}_\text{SwiGLU} = (\text{Swish}_\beta(x W_1) \otimes xV) W_2
$$
Care must be taken to ensure the weight matrices $W_1$, $W_2$ and $V$ are of the correct dimensionality, but otherwise a tensor parallel strategy is pretty straightforward: $W_1$ and $V$ are column-parallel, and $W_2$ is row-parallel. Therefore, it also has the same communication overhead.


## Un-Embedding and Loss Calculation
We un-embed with the same sharded matrix as we embed, which means we will have the outputs logits split into $TP$ chunks.

It's not mentioned in detail in the paper, but the loss calculation might be the most intrusive part in terms of code editing. 


# Communication Overhead

## Activation Checkpointing
> In Megatron-LM with activation checkpointing, each transformer block performs two allreduce operations of size batch × seq length × hidden dim in the forward propagation, **two all-reduce for forward re-computation** and two more in the backward propagation. The total communication per block is 12 × seq length × hidden dim since communication volume of an all-reduce is 2 × message size.

