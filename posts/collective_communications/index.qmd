---
title: "Collective Communications Reference"
author: "Laurens Weitkamp"
date: "2024-01-25"
draft: false

format:
  html:
    toc: true
    toc-expand: true
    toc-location: left
reference-location: margin
citation-location: margin
number-sections: true
---

A reference I made of collective communications in multiprocessing systems while working on [NuMPItron](https://github.com/lweitkamp/numpitron). There is a general guide to performant versions of each comm [here](https://web.cels.anl.gov/~thakur/papers/ijhpca-coll.pdf), although it is very specific for MPI and the same does not have to hold for GPUs. Gloo, created by Meta, also has a decent [reference guide](https://github.com/facebookincubator/gloo/blob/main/docs/algorithms.md).


# AllGather {#sec-allgather}
[NCCL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allgather) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/all_gather.py)

![](allgather.png)

- short description
- where is it used in DL

# AllReduce {#sec-allreduce}

[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/all_reduce.py)

![](allreduce.png)

Perform a reduction (`op={SUM,MAX,...}`) and broadcast to all devices. PyTorch has quite a [large amount](https://github.com/pytorch/pytorch/blob/dfc4b608e1c4343ae0a32130b5f2f47acb04fad1/torch/csrc/distributed/c10d/Types.hpp#L36-L46) of reductions available.

AllReduce can be found in **distributed data parallel**, where you sum the gradients for each weight over devices in the backward pass.

Allreduce is also found in **tensor parallel**. Here we sum each forward and backward pass of the MLP and Attention layers (and do some reductions in the embedding and loss calculation too, see [my post on tensor parallel](https://lweitkamp.github.io/posts/numpitron_tensor_parallel/)).

## Ring AllReduce
[Horovod](https://github.com/horovod/horovod), ye old distributed training framework for TensorFlow built by Uber back in the day, was too slow. Specifically, the AllReduce implementation was too slow. Originally, the algorithm implementation performed something like a [reduce - Section @sec-reduce] and [broadcast - Section @sec-broadcast] operation on the data. A single/root device would collect the data, perform the reduction, and send it to all other devices. The issue is that it creates a bandwidth bottleneck: increase the number of devices and the incoming root will not be able to receive and send data fast enough.

An engineer working at Baidu Research figured out an improvement based on a technique in high performance computing, a ring message passing scheme (the same one described in the paper above). If you picture the devices in a topological ring, each device receives data from its left neighbour and sends data to its right neighbour. In the figure below[^1] we have three devices and three chunks of data. The improved algorithm starts with a series of [ReduceScatter - Section @sec-reducescatter] operations where each device sends one of its chunk to its neighbor untill each device has one complete chunk:

![](ring_allreduce_reduce_scatter.png)

[^1]: I've adapted and simplified the figure below from the original blog post ([andrew.gibiansky.com](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/)).

With each device having a complete chunk of the data, each device now sends its neighbor the complete chunk it has untill each device has the data in full:

![](ring_allreduce_reduce_allgather.png)

This implementation makes scaling distributed data parallel much more efficient, and was probably somewhat of a milestone in distributed training frameworks.


# All to All {#sec-alltoall}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/p2p.html#all-to-all) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_to_all) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/all_to_all.py)

![](all_to_all.png)


- short description
- where is it used in DL

# Broadcast {#sec-broadcast}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#broadcast) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/broadcast.py)

![](broadcast.png)


- short description
- where is it used in DL


# Gather {#sec-gather}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#gather) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.gather) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/gather.py)

![](gather.png)


- short description
- where is it used in DL


# Receive / Send {#sec-receive}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/p2p.html#sendrecv) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.send) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/send.py)

![](sendrecv.png){width=65%}


- short description
- where is it used in DL


# Reduce {#sec-reduce}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reduce) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/reduce.py)

![](reduce.png)

Perform a reduction (`op={SUM,MAX,...}`) and send to a particular rank(`dst=RANK`).


# ReduceScatter {#sec-reducescatter}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce_scatter) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/reduce_scatter.py)

![](reducescatter.png)

- short description

From the FSDP paper: https://engineering.fb.com/2021/07/15/open-source/fsdp/attachment/fsdp-graph-2a/.


# Scatter {#sec-scatter}
[NCLL](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/p2p.html?highlight=scatter#one-to-all-scatter) | [PyTorch Docs](https://pytorch.org/docs/stable/distributed.html#torch.distributed.scatter) | [NuMPItron](https://github.com/lweitkamp/numpitron/blob/main/src/numpitron/distributed/scatter.py)

![](scatter.png)

Also known as one-to-all, an operation that shards the data block on a chosen device evenly amongst all other devices in the same group.
