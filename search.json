[
  {
    "objectID": "posts/packing/index.html",
    "href": "posts/packing/index.html",
    "title": "Packing Data for efficient Training and Inference",
    "section": "",
    "text": "The context length of transformer models is rapidly increasing with each new generation of frontier models. For instance, Google‚Äôs latest Gemini 1.5 model can process up to 1 million tokens, equivalent to 1 hour of video or approximately 700,000 words. This increase is driven by advancements in hardware and the development of more efficient algorithms tailored to the specific demands of long-context training1.\nLong contexts introduce several challenges: identifying data that inherently requires long contexts (such as books, podcasts, and videos) and efficiently handling smaller context data. Simply concatenating these documents and feeding them into an autoregressive transformer can lead to cross-contamination of sequences. For instance, predicting a sequence S2 from an unrelated sequence S1 becomes nearly impossible if there is a significant difference in their subject matter.\nThe same issue arises during inference. If we dynamically batch tokens from different users, efficiency can be improved by grouping shorter conversations in the same batch. The solution to both of these issues is a technique called packing."
  },
  {
    "objectID": "posts/packing/index.html#literature-review",
    "href": "posts/packing/index.html#literature-review",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Literature Review",
    "text": "Literature Review\nPacking is briefly described in most papers on language modelling (in fact, most authors cite T5 for it), here are some from the literature:\n\n\n\nPaper\nQuote\n\n\n\n\nRoBERTa2\n‚ÄúEach input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens.‚Äù\n\n\nGPT-33\n‚ÄúDuring training we always train on sequences of the full nctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency.‚Äù\n\n\nT54\n‚ÄúWhenever possible, we ‚Äúpack‚Äù multiple sequences into each entry of the batch so that our batches contain roughly 216 = 65,536 tokens.‚Äù\n\n\nT05\n‚Äúwe use packing to combine multiple training examples into a single sequence to reach the maximum sequence length.‚Äù\n\n\n\n2¬†RoBERTa: A Robustly Optimized BERT Pretraining Approach3¬†Language Models are Few-Shot Learners4¬†Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer5¬†Efficient Sequence Packing without Cross-contamination6¬†Efficient Sequence Packing without Cross-contaminationSome approaches use the packing described above naively and add an end-of-document token after the sequence is done to let the model figure out the difference between samples. This does seem to hurt performance due to cross-contamination mentioned before, but it might not have as big of an impact at scale6."
  },
  {
    "objectID": "posts/packing/index.html#dealing-with-cross-contamination",
    "href": "posts/packing/index.html#dealing-with-cross-contamination",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Dealing with Cross Contamination",
    "text": "Dealing with Cross Contamination\nTo prevent cross-contamination when packing sequences, it is crucial to update both the positional information and the attention masks, not just concatenating the sequences. This ensures that the attention mechanism is still valid.\n\nUpdate Positional Information\nWhen using absolute positional encodings (or ALiBi, or absolute encodings), we need to reset the positional encoding index at the start of each new sample within the packed sequence. This reset ensures that each sequence retains its positional context, which is essential for the model‚Äôs accuracy.\n\n\n\nPacked positional encodings.\n\n\nFor RoPE, you would have to do a similar rest.\n\n\nUpdate the Attention Mask\nIn self-attention mechanisms, it is necessary to ensure that one sequence cannot attend to another within the same packed context. This requires merging autoregressive masks for the self-attention layer, enforcing the restriction effectively:\n\n\n\nA packed attention mask.\n\n\nThis might be the most intrusive part to the codebase - it will require actually calculating a distinct attention mask per batch where we otherwise can simply have the standard diagonal created once for the whole batch. I imagine implementing this efficiently for ALiBi is not an enjoyable assignment.\n\n\nThat‚Äôs it!\nProperly implementing these updates ensures that the model‚Äôs gradient step remains consistent, regardless of whether sequences are processed individually or packed.\nIt‚Äôs worth your time looking at figures 3 and 4, it‚Äôs from a paper that introduces the concept of cross-contamination. The paper additionally explains the performance of packing and the effect of ‚Äòproper‚Äô masking, it‚Äôs a great read!\nAnother paper that discusses masked packing but during inference time that goes in depth on the effect of packing on prefilling and the time to first token is Prepacking."
  },
  {
    "objectID": "posts/packing/index.html#can-we-ignore-cross-contamination",
    "href": "posts/packing/index.html#can-we-ignore-cross-contamination",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Can we Ignore Cross Contamination?",
    "text": "Can we Ignore Cross Contamination?\nFrom personal experience, there is little support for packing in public codebases, often justified by the assumption that the data is not too correlated. That could be true, but it is a strong assumption to make. Interestingly, the approach in Structured Packing in LLM Training Improves Long Context Utilization essentially ignores proper masking to benefit long-context learning. However, the models are small scale and the data is not inherently long context.\nGiven that t5x does have a proper packing implementation7, I would bet that Google did some internal checks to see if it helps or not. You can choose to ignore it, but you probably don‚Äôt want to risk it at the frontier level where your infra is optimized anyway.\n\n\n7¬†They call it segmented data - from the T5x codebase."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hey, this is the website of Laurens Weitkamp. I‚Äôm a machine learning engineer interested in sequential decision making, computer vision, and multimodal models.\nI work for Thirona, where I train deep nets for medical imaging. Before that, I got my graduate degree in Artifical Intelligence where I specialized in hierarchical reinforcement learning and published a paper on explainable RL.\n\n\nRecent Posts\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPacking Data for efficient Training and Inference\n\n\nOct 1, 2024\n\n\n\n\nTensor Parallelism\n\n\nJun 12, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html",
    "href": "posts/numpitron_tensor_parallel/index.html",
    "title": "Tensor Parallelism",
    "section": "",
    "text": "Tensor parallel, introduced in the Megatron-LM paper by NVidia as intra-layer model-parallelism, is a technique for sharding model parameters across devices to reduce memory costs. It‚Äôs typically used in training large models where a single layer cannot fit into a device by itself.\nThis post explores tensor parallelism, based on my experience implementing it in NuMPItron, a small library for distributed transformer training using NumPy and MPI1. There are several posts that discuss the basics of tensor parallelism‚Äîsuch as splitting linear layers on rows or columns and dividing attention heads‚Äîbut I want to go in a bit more depth and discuss also the embedding table and the loss function.\nTensor parallelism, like pipeline model parallelism (inter-layer), requires invasive code changes2 and is as slow as the slowest device communication. Hence, it is recommended to use tensor parallelism within a single node with high-speed communication like NVLink. Typically, \\(N_\\text{tp} = 8\\), splitting the model weights over 8 devices, is the default for large runs."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#input-embedding",
    "href": "posts/numpitron_tensor_parallel/index.html#input-embedding",
    "title": "Tensor Parallelism",
    "section": "Input Embedding",
    "text": "Input Embedding\nThe embedding layer takes a token index, locates the corresponding row in the embedding table, and returns that \\(D_\\text{model}\\) dimensional row. In a tensor parallel model, the vocab dimension (rows) of \\(N_\\text{vocab}\\) are divided into chunks across devices.\nAn issue arises when a device encounters a token index outside its chunk, which can be solved by masking. Each device returns a tensor of size (batch size, sequence length, \\(D_\\text{model}\\)), filling out-of-bounds tokens for its chunk with zeros. Once each device completes this step, an ALLREDUCE operation ensures that masked indices are properly filled across all devices.\nThe figure below visualizes this process. If you‚Äôd rather see it implemented, refer to the NuMPItron codebase.\n\n\n\nAll tokens are present on all devices. The embedding forward pass performs a masked-lookup per device and the results are ALLREDUCE‚Äôd.\n\n\nAre there any alternatives to this? Why wouldn‚Äôt we slice the tensor on the \\(D_\\text{model}\\) dim instead? I think this would be straightforward - each device would create a full batch size by sequence length by \\(\\frac{D_\\text{model}}{N_\\text{tp}}\\) tensor. It wouldn‚Äôt require masking, but it would require an ALLGATHER, which has a worse worse complexity vs ALLREDUCE.\nThere are no reduction in the backward pass, but you will need to ensure that the correct indices in the embedding table are being updated by masking."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#attention",
    "href": "posts/numpitron_tensor_parallel/index.html#attention",
    "title": "Tensor Parallelism",
    "section": "Attention",
    "text": "Attention\nThe attention layer transforms the inputs into Query, Key, and Value tensors with weights \\(W_Q, W_K, W_V,\\) each of shape \\(D_\\text{model} \\times n_\\text{heads} \\times d_\\text{head}\\). Each head calculates the attention weights and the attention values separately, and projects it back to a shared representation with weight matrix \\(W_O\\) of shape \\(d_\\text{head} \\times n_\\text{heads} \\times D_\\text{model}\\).\nBecause computation is done separately per head, tensor parallel distributes the heads over the devices. It‚Äôs depicted somewhat simplified below3:\n3¬†Each token has several attention heads, this is still reflected in the attention weights but picture seq_len outputs.\n\n\nQuery, key, value, and output heads are created sharded over devices. Attention is calculated per head and transformed to an output value which is ALLREDUCEd.\n\n\nThis ensures that the softmax attention weights are valid. Each device will therefore have in total \\(4 \\times D_\\text{model} \\times \\frac{N_\\text{heads}}{2} \\times d_\\text{head}\\) weights in memory (we assume no biases for the attention layer). An ALLREDUCE collects the output per device at the end of the forward pass and also at the end of the backward pass."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#groupedmulti-query-attention",
    "href": "posts/numpitron_tensor_parallel/index.html#groupedmulti-query-attention",
    "title": "Tensor Parallelism",
    "section": "Grouped/Multi-Query Attention",
    "text": "Grouped/Multi-Query Attention\nMulti-query attention (MQA) is a modification to the transformer model where the key and value head size is reduced from \\(N_\\text{heads}\\) to \\(1\\) to vastly reduce memory consumption when decoding. Grouped query attention (GQA) was introduced more recently as an interpolation between vanilla and multi-query attention to ensure quality does not degrade too much by the reduction in total parameters.\nWe have a bit of an issue when using MQA with \\(N_\\text{TP}=8\\), since we can‚Äôt divide the single head to each device. A practical solution to this is to replicate the head to \\(N_\\text{TP}\\) size effectively using the same head on each device. In general you will see that GQA is used more often and it is set to 8 specifically to serve 8 devices in parallel4.\n4¬†Trainium devices have similar issue since they come in 32 per node, they also advise to replicate the heads: https://awsdocs-neuron.readthedocs-hostedlcom/en/latest/libraries/neuronx-distributed/api_guide.html#gqa-qkv-linear-module."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#feed-forward-network",
    "href": "posts/numpitron_tensor_parallel/index.html#feed-forward-network",
    "title": "Tensor Parallelism",
    "section": "Feed Forward Network",
    "text": "Feed Forward Network\nThe feed forward net is a simple two layer multi layer perceptron with a ReLU nonlinearity in the middle: \\[\n\\large \\text{FFN}_\\text{MLP} = \\max(xW_1 + b_1, 0)W_2 + b_2\n\\] This means we have two weight matrices including biases that we need to shard across devices in some way that makes mathematical sense.\nDepicted below is a 2D matrix multiplication, something like \\(xW\\). We can shard computation along its columns (‚Äúcolumn-parallel‚Äù) or along its rows (‚Äúrow-parallel‚Äù).\n\n\n\nA generic matrix multiplication.\n\n\nIf we do a column-parallel sharding strategy, we end up with complete sums but the results are sharded across devices, requiring an ALLGATHER operation.\n\n\n\nA column-parallel matrix multiplication.\n\n\nLooking at the row-parallel strategy instead, we end up with partial sums across devices that require an ALLREDUCE.\n\n\n\nA row-parallel matrix multiplication.\n\n\nFollowing the matrix multiplication with a ReLU or any nonlinearity, we can see that row-parallel will have some issues here, \\(\\text{ReLU}(-5) + \\text{ReLU}(14) \\neq \\text{ReLU}(9)\\). Performing a column-parallel strategy first ensures we can perform any nonlinearity since the values are complete already. We can follow it with a row-parallel matrix multiplication and ALLREDUCE the results for minimal communication overhead. An additional AllREDUCE is required at the end of the backward pass too.\nFor the bias terms, we need to ensure that the column-parallel multiplication adds the bias only on a single device, and for the row-parallel multiplication we can split the bias.\n\nSwiGLU\nSwish Gated Linear Units (SwiGLU) combine the Swish activation function5 with a Gated Linear Unit: a component-wise product of two linear layers. The result is that we have three weight matrices instead of two, and we omit any bias:\n5¬†. \\(\\text{Swish}_\\beta(x) = x \\sigma (\\beta x)\\) where \\(\\sigma(x)\\) is the sigmoid activation function.\\[\n\\large \\text{FFN}_\\text{SwiGLU} = (\\text{Swish}_\\beta(x W_1) \\otimes xV) W_2\n\\]\nImplementations tend to try and ensure that the SwiGLU does not lose any parameters when compared to the vanilla MLP6, but otherwise a tensor parallel strategy is pretty straightforward: \\(W_1\\) and \\(V\\) are column-parallel, and \\(W_2\\) is row-parallel. Therefore, it also has the same communication overhead.\n6¬†See The Case for Co-Designing Model Architectures with Hardware. SwiGLU recommends \\(\\frac{8}{3}d_\\text{head}\\) instead of the MLPs \\(4 \\cdot D_\\text{model}\\), but this default is sub-optimal."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#un-embedding-and-loss-calculation",
    "href": "posts/numpitron_tensor_parallel/index.html#un-embedding-and-loss-calculation",
    "title": "Tensor Parallelism",
    "section": "Un-Embedding and Loss Calculation",
    "text": "Un-Embedding and Loss Calculation\nSince Megatron-LM assumes tied-embeddings, we un-embed with the same sharded matrix as we embed, which means the output logits will be split into \\(N_\\text{TP}\\) chunks. There are no reductions for the un-embedding layer in the forward pass, but you will need to ALLREDUCE the gradients at the end of the backward pass.\nUnlike other layers, which only required a smart initialization upfront (dividing head dimensions, rows, or columns by the number of devices) and some all-reduce operations afterward, the loss calculation is more complex. After un-embedding, each token has a set of logit predictions of \\(N_\\text{vocab}\\) length.\n\n\n\nLogit activation magnitude per token. logits are sharded over devices along the vocab dimension.\n\n\nOur goal is to compute the softmax cross-entropy loss function7:\n7¬†global batch size = number of microbatches * microbatch size * \\(N_\\text{DP}\\)\\[\n\\large \\text{loss} = - \\log (\\text{logit}_\\text{true label}) + \\log(\\sum \\exp(\\text{all logits}))\n\\]\nassuming the maximum value has already been subtracted for numerical stability. The first step is to calculate the max logit value per token per device using an ALLREDUCE. This is a relatively cheap operation requiring communication of only \\(N_\\text{batch} \\times N_\\text{ctx}\\) values.\n\n\n\nThere are three reductions in the loss calculation. The first is an ALLREDUCE-max logit value for a stable loss calculation. The second is a masked ALLREDUCE such that each device has the labeled logit activation. The third and last ALLREDUCE ensures the log-sum-exp value is the same on all devices.\n\n\nNext, we communicate the logit of the true label to all devices. This is done by checking if the label index is within the current device‚Äôs chunk and masking it if it is not. An ALLREDUCE fills any masked locations, ensuring each device has the logit of the true label. The final communication step is the sum calculation: exponentiate the original logits per device, sum them, and perform the last ALLREDUCE of the day.\nAll in all, that‚Äôs a lot of reductions happening. Notably, however, is the fact that each of the reductions only communicates \\(N_\\text{batch} \\times N_\\text{ctx}\\) values. I guess it wasn‚Äôt worth going into detail about it in the Megatron-LM paper.\nThere are ways to reduce computational overhead by calculating the softmax required for the backward pass (we essentially have everything ready after communicating the log-sum-exp values) and even looking into fused cross-entropy8, but that is basically it.\n8¬†https://github.com/mgmalek/efficient_cross_entropy."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#communication-overhead",
    "href": "posts/numpitron_tensor_parallel/index.html#communication-overhead",
    "title": "Tensor Parallelism",
    "section": "Communication Overhead",
    "text": "Communication Overhead\nThe table below gives us the forward and backward pass communications required across devices for the Megatron-LM style tensor parallel transformer.\n\n\n\n\n\n\n\n\nLayer\nForward Pass\nBackward Pass\n\n\n\n\nInput Embedding\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\nN/A\n\n\nAttention\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\n\nMLP\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\n\nOutput Embedding\nN/A\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\n\nCross-Entropy\n\\(3 \\cdot N_\\text{batch} \\times N_\\text{ctx}\\)\nN/A\n\n\n\nWe need to keep in mind that the Attention and MLP layers are most important here, since these will happen at every layer. It does seem to scale quite bad in terms of sequence length, something I might cover in an upcoming post when I add sequence parallel to NuMPItron.\n\nActivation Checkpointing\nHidden activations required for backward-pass calculations can quickly fill up device memory, and the larger the model size the faster memory fills up. To alleviate this somewhat, you can use activation checkpointing, which re-calculates the hidden activations during the backward pass by storing only the layer input instead.\nThis is a very useful technique to use, but when used naively it requires two additional ALLREDUCE operations when re-calculating the forward pass activations for the backward pass. The Megatron-LM authors fixed this in favor of a more selective recomputation in the sequence parallel paper9.\n9¬†See Reducing Activation Recomputation in Large Transformer Models."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#pipeline-and-data-parallel",
    "href": "posts/numpitron_tensor_parallel/index.html#pipeline-and-data-parallel",
    "title": "Tensor Parallelism",
    "section": "Pipeline and Data Parallel",
    "text": "Pipeline and Data Parallel\nPipeline parallelism splits the layers into stages, forwarding data between them. A common issue with this approach is the potential for ‚Äòbubbles‚Äô of inactivity. To mitigate this, data is sent in ‚Äòmicrobatches.‚Äô For instance, if the batch size is 8 and the microbatch size is 4, two microbatches of size 4 are sent. Increasing the number of microbatches reduces the bubble but generally also decreases the microbatch size.\n\n\n\nA very basic view of pipeline parallel. As we increase the number of microbatches, both the microbatch size and the bubble decreases. device 1 calculates the forward pass of one microbatch and forwards it to device 2, who calculates the forward pass and backwards pass before sending it back.\n\n\nThe number of pipeline stages determines the device spread, and the number of microbatches informs the global batch size10 needed for one forward and backward step. We want to know both! Unfortunately, Meta did not provide this information. We can, however, derive the data parallel value from reasonable default cases of pipeline parallel11.\n10¬†global batch size = number of microbatches * microbatch size * \\(N_\\text{DP}\\)11¬†To reduce the search space, we will set some good baseline values: the microbatch size to 1 (low bubble), the number of microbatches are a ‚Äòreasonable‚Äô multiple of the number of pipeline layers.\n\n\n\n\n\n\n\n\n\n\\(N_\\text{PP}\\)\n\\(N_\\text{DP}\\)\nnumber of microbatches\nGlobal batch size\nGlobal batch size \\(\\times\\) sequence length\n\n\n\n\n4\n512\n8\n4096\n33M\n\n\n4\n512\n12\n6144\n50M\n\n\n4\n512\n16\n8192\n67M\n\n\n8\n256\n12\n3072\n25M\n\n\n8\n256\n16\n4096\n33M\n\n\n8\n256\n20\n5120\n42M\n\n\n16\n128\n20\n2560\n21M\n\n\n16\n128\n24\n3072\n25M\n\n\n16\n128\n28\n3584\n30M\n\n\n\nAs we see, with an increase of \\(N_\\text{TP}\\) we decrease the global batch size potential. Some of these values look huge, but what can we compare it with:\n\nLLaMA-2-70B was trained on 2B global batch size in tokens\nDeepseek-67B was trained on 19.4M batch size in tokens\nOPT-175B was trained on 2M batch size in tokens\n\nAs an aside, pipeline parallelism is not required in combination with data and tensor parallel, OPT-175B was trained using only tensor-parallel and FSDP on 992 GPUs12.\n12¬†From the log-book, we have \\(N_\\text{TP}=8\\) on 992 GPUs, leaving 124 groups of 8 GPUs for data parallel training. We also have a local batch size of 8 for each group and a context length of 2048. Multiply 128 group by 8 local batch size by 2048 context length and we have 2031616, roughly 2M."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#a-visual-example",
    "href": "posts/numpitron_tensor_parallel/index.html#a-visual-example",
    "title": "Tensor Parallelism",
    "section": "A Visual Example",
    "text": "A Visual Example\nIt‚Äôs hard to visualize over 16K GPUs sharding a model, so let‚Äôs restrict ourselves to the following setup: \\(N_\\text{TP}=4\\), \\(N_\\text{PP} = 2\\) and \\(n_\\text{DP}=2\\). That is, four nodes with 4 GPUs each with two pipeline stages and 2 data parallel stages. Intra-node tensor parallelism is fast, we can have a large number of microbatches to ensure communication between nodes does not bother us and data parallel is essentially the cheapest parallelization techniques in terms of overhead:\n\n\n\n\\(N_\\text{TP}=4\\), \\(N_\\text{PP} = 2\\) and \\(n_\\text{DP}=2\\). A global batch gets sharded across devices in data parallel mode, the first pipeline layer has half of the transformer weights and uses tensor parallel for fast communication, after which it sends it the data to the second pipeline layer.\n\n\nThat wraps it up for this post. I hope this post helped you understand tensor parallel and perhaps even 3D parallel a bit better. Please let me know what you think, you can contact me on ùïè. The next post will either discuss pipeline parallel in more depth or distributed sampling strategies, after implementing it in NuMPItron ofcourse."
  }
]