[
  {
    "objectID": "posts/packing/index.html",
    "href": "posts/packing/index.html",
    "title": "Packing Data for efficient Training and Inference",
    "section": "",
    "text": "The context length of transformer models is rapidly increasing with each new generation of frontier models. For instance, Google’s latest Gemini 1.5 model can process up to 1 million tokens, equivalent to 1 hour of video or approximately 700,000 words. This increase is driven by advancements in hardware and the development of more efficient algorithms tailored to the specific demands of long-context training1.\nLong contexts introduce several challenges: identifying data that inherently requires long contexts (such as books, podcasts, and videos) and efficiently handling smaller context data. Simply concatenating these documents and feeding them into an autoregressive transformer can lead to cross-contamination of sequences. For instance, predicting a sequence S2 from an unrelated sequence S1 becomes nearly impossible if there is a significant difference in their subject matter.\nThe same issue arises during inference. If we dynamically batch tokens from different users, efficiency can be improved by grouping shorter conversations in the same batch. The solution to both of these issues is a technique called packing."
  },
  {
    "objectID": "posts/packing/index.html#literature-review",
    "href": "posts/packing/index.html#literature-review",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Literature Review",
    "text": "Literature Review\nPacking is briefly described in most papers on language modelling (in fact, most authors cite T5 for it), here are some from the literature:\n\n\n\nPaper\nQuote\n\n\n\n\nRoBERTa2\n“Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens.”\n\n\nGPT-33\n“During training we always train on sequences of the full nctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency.”\n\n\nT54\n“Whenever possible, we “pack” multiple sequences into each entry of the batch so that our batches contain roughly 216 = 65,536 tokens.”\n\n\nT05\n“we use packing to combine multiple training examples into a single sequence to reach the maximum sequence length.”\n\n\n\n2 RoBERTa: A Robustly Optimized BERT Pretraining Approach3 Language Models are Few-Shot Learners4 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer5 Efficient Sequence Packing without Cross-contamination6 Efficient Sequence Packing without Cross-contaminationSome approaches use the packing described above naively and add an end-of-document token after the sequence is done to let the model figure out the difference between samples. This does seem to hurt performance due to cross-contamination mentioned before, but it might not have as big of an impact at scale6."
  },
  {
    "objectID": "posts/packing/index.html#dealing-with-cross-contamination",
    "href": "posts/packing/index.html#dealing-with-cross-contamination",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Dealing with Cross Contamination",
    "text": "Dealing with Cross Contamination\nTo prevent cross-contamination when packing sequences, it is crucial to update both the positional information and the attention masks, not just concatenating the sequences. This ensures that the attention mechanism is still valid.\n\nUpdate Positional Information\nWhen using absolute positional encodings (or ALiBi, or absolute encodings), we need to reset the positional encoding index at the start of each new sample within the packed sequence. This reset ensures that each sequence retains its positional context, which is essential for the model’s accuracy.\n\n\n\nPacked positional encodings.\n\n\nFor other positional encoding methods, such as RoPE, similar adjustments can be made. For RoPE, this involves resetting the rotation angles.\n\n\nUpdate the Attention Mask\nIn self-attention mechanisms, it is necessary to ensure that one sequence cannot attend to another within the same packed context. This requires merging autoregressive masks for the self-attention layer, enforcing the restriction effectively:\n\n\n\nA packed attention mask.\n\n\nThis might be the most intrusive part to the codebase - it will require actually calculating a distinct attention mask per batch where we otherwise can simply have the standard diagonal created once for the whole batch. I imagine implementing this efficiently for ALiBi is not an enjoyable assignment.\n\n\nThat’s it!\nProperly implementing these updates ensures that the model’s gradient step remains consistent, regardless of whether sequences are processed individually or packed.\nIt’s worth your time looking at figures 3 and 4, it’s from a paper that introduces the concept of cross-contamination. The paper additionally explains the performance of packing and the effect of ‘proper’ masking, it’s a great read!\nAnother paper that discusses masked packing but during inference time that goes in depth on the effect of packing on prefilling and the time to first token is Prepacking."
  },
  {
    "objectID": "posts/packing/index.html#can-we-ignore-cross-contamination",
    "href": "posts/packing/index.html#can-we-ignore-cross-contamination",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Can we Ignore Cross Contamination?",
    "text": "Can we Ignore Cross Contamination?\nFrom personal experience, there is little support for packing in public codebases, often justified by the assumption that the data is not too correlated. Interestingly, the approach in Structured Packing in LLM Training Improves Long Context Utilization essentially ignores proper masking to benefit long-context learning.\nIt is interesting to see that GPT-3 did not use masked packing (from the quote above). It is quite possible that these cross contamination performance issues are not present in large scale training. Ofcourse, GPT-4 might be trained by masked packing, but that is not publicly disclosed.\nDoes Gemini/PaLM use masked packing? If they use t5x it seems so7:\n7 They call it segmented data - from the T5x codebase.\nA sequence length of 2048 was used for all models. Input examples are concatenated together and then split into sequences of exactly 2048 tokens, so that there are no padding tokens, but examples may be split in the middle. Input examples are differentiated from one another with a special [eod] token."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hey, this is the website of Laurens Weitkamp. I’m a machine learning engineer interested in sequential decision making, computer vision, and multimodal models.\nI work for Thirona, where I train deep nets for medical imaging. Before that, I got my graduate degree in Artifical Intelligence where I specialized in hierarchical reinforcement learning and published a paper on explainable RL.\n\n\nRecent Posts\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPacking Data for efficient Training and Inference\n\n\nJun 10, 2024\n\n\n\n\n\nNo matching items"
  }
]