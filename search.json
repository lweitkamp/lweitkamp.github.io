[
  {
    "objectID": "posts/qlora_dequantize/index.html",
    "href": "posts/qlora_dequantize/index.html",
    "title": "QLoRA Weight Dequantizing in Triton",
    "section": "",
    "text": "Unsloth has released a list of some challenging tasks related to PyTorch and parameter efficient fine-tuning. One of the challenges was writing a Triton kernel for dequantizing QLoRA NF4 quantized weights. I thought it would be a fun challenge on a technical level to go beyond tutorial-like kernels for softmax and matrix multiplications1.\nIn this post2, I’ll walk through the process of converting CUDA code to Triton and share how I achieved performance improvements of up to 1.6X to 1.8X for LLaMA models ranging from 6B to 65B parameters. Notably, the speed gains from my implementation increase as model size scales up, making this approach particularly valuable for larger models — which is basically the intended use for QLoRA.\nWe start with discussing what QLoRA is and move on to the CUDA kernels that are typically launched by bitsandybtes, the library used for NF4 quantization in Unsloth and PEFT. From there, we mimic the CUDA kernels in Triton and benchmark its performance. We sort of know that the CUDA kernels can be fused in Triton and use this to our advantage to create a single fused kernel which turns out to be slower! After benchmarking with Nsight compute, we notice that memory coalescing is the offender and fix it in three distinct approaches that all outperform its CUDA counterpart."
  },
  {
    "objectID": "posts/qlora_dequantize/index.html#the-normalfloat-nf4-data-type",
    "href": "posts/qlora_dequantize/index.html#the-normalfloat-nf4-data-type",
    "title": "QLoRA Weight Dequantizing in Triton",
    "section": "The NormalFloat (NF4) Data Type",
    "text": "The NormalFloat (NF4) Data Type\nNF4 uses 4 bits as an index to a 16 element lookup table. Since there is no 4 bit data type in Torch, we have to pack and unpack 4 bits from a torch.uint8 type.\n&gt;&gt;&gt; packed = torch.zeros((), dtype=torch.uint8)\n&gt;&gt;&gt; packed                      # [0 0 0 0 | 0 0 0 0]\n1&gt;&gt;&gt; packed = packed + 5         # [0 0 0 0 | 0 1 0 1]\n2&gt;&gt;&gt; packed = (packed &lt;&lt; 4) + 9  # [0 1 0 1 | 1 0 0 1]\n3&gt;&gt;&gt; first = (packed &gt;&gt; 4) & 0xF # [0 0 0 0 | 0 1 0 1] (5)\n&gt;&gt;&gt; second = packed & 0xF       # [0 0 0 0 | 1 0 0 1] (9)\n\n1\n\nWe start with a torch.uint8 zeros singleton tensor and add a 5 to it.\n\n2\n\nWe add another 4 bit value (9), but we have to shift the 4 least significant bits to make room.\n\n3\n\nTo retrieve the packed values, we use a combination of masking (0xF = [0 0 0 0 1 1 1 1]) and bit-shifting.\n\n\n4 bits (or 16 elements) is not a lot to work with, so the authors of QLoRA take advantage of the following:\n\nSince pretrained neural network weights usually have a zero-centered normal distribution with standard deviation σ (see Appendix F3), we can transform all weights to a single fixed distribution by scaling σ such that the distribution fits exactly into the range of our data type.\n3 Appendix F leads to a proof in the form of a statistical test.\nThe resulting lookup table is a 16 element table of fp32 values4:\n4 You can find the same in Appendix E.nf4_code_table = torch.tensor([\n  -1.0,\n  -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,\n  -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, \n1  0.0,\n  0.07958029955625534, 0.16093020141124725, 0.24611230194568634,\n2  0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023,\n  1.0,\n], dtype=torch.float32)\n\n1\n\nThe table has a true zero point and a maximum range of [-1, 1].\n\n2\n\nThere is one more positive value to ensure the zero point.\n\n\nThe elements here are carefully constructed to represent a Normal distribution (hence NormalFloat). The approach is straightforward: create quantiles of the Normal distribution based on the data type (\\(k\\)-bits, in our case \\(k=4\\)) and ensure there is a true zero point and that it is bounded by [-1, 1]:\n\n\n\nNormal distribution with 16 quantiles. Values and indices match the code table.\n\n\n\nQuantization & Dequantization\nThe weights are quantized in successive blocks of 64 ensuring a high precision (and accounting for outlier values). This means that successive 64 weights have the same absolute-maximum scaling factor5, which is stored in fp32. At this point we quantize “as usual” - find the closest code in the lookup table nf4_code_table to the weight and use the index of that code as the NF4 value that we pack. For this scale factor we will stick to the term absmax.\n5 weight = quantized_weight × block_scale_factor, scale factor essentially normalizes to [-1, 1].Dequantization is the easier part. From a packed torch.uint8 byte we extract two NF4 indices and look-up the corresponding code in the table nf4_code_table. Then scale it by retrieving the corresponding absmax value for this weight index - the packed value will have the same absmax value, but its neighbour weights might not."
  },
  {
    "objectID": "posts/qlora_dequantize/index.html#double-quantization",
    "href": "posts/qlora_dequantize/index.html#double-quantization",
    "title": "QLoRA Weight Dequantizing in Triton",
    "section": "Double Quantization",
    "text": "Double Quantization\nWith the NF4 quantization strategy above, let’s take a look at a 7B model. 7B in FP16 terms is about 14GBs of storage, and roughly 3.5GBs when quantized to NF4 - that’s a huge decrease! But we have to account for the absmax storage, roughly 100 million of them (7B/64), and all in fp32 - that adds almost half a GB of storage. We could reduce the number of absmax factors by increasing the block size, but this turns out to have negative effects on quantization performance.\nTo reduce the overhead of the absmax storage, QLoRA does a double quantization - quantize the absmax from fp32 to fp8. Quantization here is purely to a byte level so no packing and unpacking is involved here, and the type of quantization is fp86. The approach to quantization is to first subtract the mean from absmax in total (since the values are all positive), take blocks of 256 (a bit courser here for large compression gains) and quantize them to a code table of 128 fp32 values.\n6 I’m not sure exactly how it works, but it is using a dynamic quantization map - another type of code.To dequantize the absmax we approximately follow the code below.\nabsmax[  0] = code[quantized_absmax[  0]] * absmax_scale[0] + absmax_mean\nabsmax[  1] = code[quantized_absmax[  1]] * absmax_scale[0] + absmax_mean\n...\nabsmax[255] = code[quantized_absmax[255]] * absmax_scale[0] + absmax_mean\n1absmax[256] = code[quantized_absmax[256]] * absmax_scale[1] + absmax_mean\n\n1\n\nNote the same scale is valid per 256 blocks."
  },
  {
    "objectID": "posts/qlora_dequantize/index.html#what-gain-could-a-triton-kernel-have",
    "href": "posts/qlora_dequantize/index.html#what-gain-could-a-triton-kernel-have",
    "title": "QLoRA Weight Dequantizing in Triton",
    "section": "What Gain Could a Triton Kernel Have?",
    "text": "What Gain Could a Triton Kernel Have?\nHaving looked at the CUDA kernel, what kind of gain in performance can we expect when switching to Triton? I suspect the following two points could be used:\n\nWe can get rid of all the overhead the kernel adds, but this might not be a tremendous gain\nWe can ultimately fuse the two kernels, since the absmax value is shared by a lot of threads.\n\nFleshing point 2 out a bit more, we can load a block of the weights, get the corresponding absmax and dequantize it in a blockwise fashion before dequantizing the weights. The more I write this the more I think that there is a lot of locality referencing going on with the absmax, even if the dequantization itself is just an elementwise operation. The only downside I see is that we probably will dequantize the same absmax several times."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html",
    "href": "posts/numpitron_tensor_parallel/index.html",
    "title": "Tensor Parallelism",
    "section": "",
    "text": "Tensor parallel, introduced in the Megatron-LM paper by NVidia as intra-layer model-parallelism, is a technique for sharding model parameters across devices to reduce memory costs. It’s typically used in training large models where a single layer cannot fit into a device by itself.\nThis post explores tensor parallelism, based on my experience implementing it in NuMPItron, a small library for distributed transformer training using NumPy and MPI1. There are several posts that discuss the basics of tensor parallelism—such as splitting linear layers on rows or columns and dividing attention heads—but I want to go in a bit more depth and discuss also the embedding table and the loss function.\nTensor parallelism, like pipeline model parallelism (inter-layer), requires invasive code changes2 and is as slow as the slowest device communication. Hence, it is recommended to use tensor parallelism within a single node with high-speed communication like NVLink. Typically, \\(N_\\text{tp} = 8\\), splitting the model weights over 8 devices, is the default for large runs."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#input-embedding",
    "href": "posts/numpitron_tensor_parallel/index.html#input-embedding",
    "title": "Tensor Parallelism",
    "section": "Input Embedding",
    "text": "Input Embedding\nThe embedding layer takes a token index, locates the corresponding row in the embedding table, and returns that \\(D_\\text{model}\\) dimensional row. In a tensor parallel model, the vocab dimension (rows) of \\(N_\\text{vocab}\\) are divided into chunks across devices.\nAn issue arises when a device encounters a token index outside its chunk, which can be solved by masking. Each device returns a tensor of size (batch size, sequence length, \\(D_\\text{model}\\)), filling out-of-bounds tokens for its chunk with zeros. Once each device completes this step, an ALLREDUCE operation ensures that masked indices are properly filled across all devices.\nThe figure below visualizes this process. If you’d rather see it implemented, refer to the NuMPItron codebase.\n\n\n\nAll tokens are present on all devices. The embedding forward pass performs a masked-lookup per device and the results are ALLREDUCE’d.\n\n\nAre there any alternatives to this? Why wouldn’t we slice the tensor on the \\(D_\\text{model}\\) dim instead? I think this would be straightforward - each device would create a full batch size by sequence length by \\(\\frac{D_\\text{model}}{N_\\text{tp}}\\) tensor. It wouldn’t require masking, but it would require an ALLGATHER, which has a worse worse complexity vs ALLREDUCE.\nThere are no reduction in the backward pass, but you will need to ensure that the correct indices in the embedding table are being updated by masking."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#attention",
    "href": "posts/numpitron_tensor_parallel/index.html#attention",
    "title": "Tensor Parallelism",
    "section": "Attention",
    "text": "Attention\nThe attention layer transforms the inputs into Query, Key, and Value tensors with weights \\(W_Q, W_K, W_V,\\) each of shape \\(D_\\text{model} \\times n_\\text{heads} \\times d_\\text{head}\\). Each head calculates the attention weights and the attention values separately, and projects it back to a shared representation with weight matrix \\(W_O\\) of shape \\(d_\\text{head} \\times n_\\text{heads} \\times D_\\text{model}\\).\nBecause computation is done separately per head, tensor parallel distributes the heads over the devices. It’s depicted somewhat simplified below3:\n3 Each token has several attention heads, this is still reflected in the attention weights but picture seq_len outputs.\n\n\nQuery, key, value, and output heads are created sharded over devices. Attention is calculated per head and transformed to an output value which is ALLREDUCEd.\n\n\nThis ensures that the softmax attention weights are valid. Each device will therefore have in total \\(4 \\times D_\\text{model} \\times \\frac{N_\\text{heads}}{2} \\times d_\\text{head}\\) weights in memory (we assume no biases for the attention layer). An ALLREDUCE collects the output per device at the end of the forward pass and also at the end of the backward pass."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#groupedmulti-query-attention",
    "href": "posts/numpitron_tensor_parallel/index.html#groupedmulti-query-attention",
    "title": "Tensor Parallelism",
    "section": "Grouped/Multi-Query Attention",
    "text": "Grouped/Multi-Query Attention\nMulti-query attention (MQA) is a modification to the transformer model where the key and value head size is reduced from \\(N_\\text{heads}\\) to \\(1\\) to vastly reduce memory consumption when decoding. Grouped query attention (GQA) was introduced more recently as an interpolation between vanilla and multi-query attention to ensure quality does not degrade too much by the reduction in total parameters.\nWe have a bit of an issue when using MQA with \\(N_\\text{TP}=8\\), since we can’t divide the single head to each device. A practical solution to this is to replicate the head to \\(N_\\text{TP}\\) size effectively using the same head on each device. In general you will see that GQA is used more often and it is set to 8 specifically to serve 8 devices in parallel4.\n4 Trainium devices have similar issue since they come in 32 per node, they also advise to replicate the heads: https://awsdocs-neuron.readthedocs-hostedlcom/en/latest/libraries/neuronx-distributed/api_guide.html#gqa-qkv-linear-module."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#feed-forward-network",
    "href": "posts/numpitron_tensor_parallel/index.html#feed-forward-network",
    "title": "Tensor Parallelism",
    "section": "Feed Forward Network",
    "text": "Feed Forward Network\nThe feed forward net is a simple two layer multi layer perceptron with a ReLU nonlinearity in the middle: \\[\n\\large \\text{FFN}_\\text{MLP} = \\max(xW_1 + b_1, 0)W_2 + b_2\n\\] This means we have two weight matrices including biases that we need to shard across devices in some way that makes mathematical sense.\nDepicted below is a 2D matrix multiplication, something like \\(xW\\). We can shard computation along its columns (“column-parallel”) or along its rows (“row-parallel”).\n\n\n\nA generic matrix multiplication.\n\n\nIf we do a column-parallel sharding strategy, we end up with complete sums but the results are sharded across devices, requiring an ALLGATHER operation.\n\n\n\nA column-parallel matrix multiplication.\n\n\nLooking at the row-parallel strategy instead, we end up with partial sums across devices that require an ALLREDUCE.\n\n\n\nA row-parallel matrix multiplication.\n\n\nFollowing the matrix multiplication with a ReLU or any nonlinearity, we can see that row-parallel will have some issues here, \\(\\text{ReLU}(-5) + \\text{ReLU}(14) \\neq \\text{ReLU}(9)\\). Performing a column-parallel strategy first ensures we can perform any nonlinearity since the values are complete already. We can follow it with a row-parallel matrix multiplication and ALLREDUCE the results for minimal communication overhead. An additional AllREDUCE is required at the end of the backward pass too.\nFor the bias terms, we need to ensure that the column-parallel multiplication adds the bias only on a single device, and for the row-parallel multiplication we can split the bias.\n\nSwiGLU\nSwish Gated Linear Units (SwiGLU) combine the Swish activation function5 with a Gated Linear Unit: a component-wise product of two linear layers. The result is that we have three weight matrices instead of two, and we omit any bias:\n5 . \\(\\text{Swish}_\\beta(x) = x \\sigma (\\beta x)\\) where \\(\\sigma(x)\\) is the sigmoid activation function.\\[\n\\large \\text{FFN}_\\text{SwiGLU} = (\\text{Swish}_\\beta(x W_1) \\otimes xV) W_2\n\\]\nImplementations tend to try and ensure that the SwiGLU does not lose any parameters when compared to the vanilla MLP6, but otherwise a tensor parallel strategy is pretty straightforward: \\(W_1\\) and \\(V\\) are column-parallel, and \\(W_2\\) is row-parallel. Therefore, it also has the same communication overhead.\n6 See The Case for Co-Designing Model Architectures with Hardware. SwiGLU recommends \\(\\frac{8}{3}d_\\text{head}\\) instead of the MLPs \\(4 \\cdot D_\\text{model}\\), but this default is sub-optimal."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#un-embedding-and-loss-calculation",
    "href": "posts/numpitron_tensor_parallel/index.html#un-embedding-and-loss-calculation",
    "title": "Tensor Parallelism",
    "section": "Un-Embedding and Loss Calculation",
    "text": "Un-Embedding and Loss Calculation\nSince Megatron-LM assumes tied-embeddings, we un-embed with the same sharded matrix as we embed, which means the output logits will be split into \\(N_\\text{TP}\\) chunks. There are no reductions for the un-embedding layer in the forward pass, but you will need to ALLREDUCE the gradients at the end of the backward pass.\nUnlike other layers, which only required a smart initialization upfront (dividing head dimensions, rows, or columns by the number of devices) and some all-reduce operations afterward, the loss calculation is more complex. After un-embedding, each token has a set of logit predictions of \\(N_\\text{vocab}\\) length.\n\n\n\nLogit activation magnitude per token. logits are sharded over devices along the vocab dimension.\n\n\nOur goal is to compute the softmax cross-entropy loss function7:\n7 global batch size = number of microbatches * microbatch size * \\(N_\\text{DP}\\)\\[\n\\large \\text{loss} = - \\log (\\text{logit}_\\text{true label}) + \\log(\\sum \\exp(\\text{all logits}))\n\\]\nassuming the maximum value has already been subtracted for numerical stability. The first step is to calculate the max logit value per token per device using an ALLREDUCE. This is a relatively cheap operation requiring communication of only \\(N_\\text{batch} \\times N_\\text{ctx}\\) values.\n\n\n\nThere are three reductions in the loss calculation. The first is an ALLREDUCE-max logit value for a stable loss calculation. The second is a masked ALLREDUCE such that each device has the labeled logit activation. The third and last ALLREDUCE ensures the log-sum-exp value is the same on all devices.\n\n\nNext, we communicate the logit of the true label to all devices. This is done by checking if the label index is within the current device’s chunk and masking it if it is not. An ALLREDUCE fills any masked locations, ensuring each device has the logit of the true label. The final communication step is the sum calculation: exponentiate the original logits per device, sum them, and perform the last ALLREDUCE of the day.\nAll in all, that’s a lot of reductions happening. Notably, however, is the fact that each of the reductions only communicates \\(N_\\text{batch} \\times N_\\text{ctx}\\) values. I guess it wasn’t worth going into detail about it in the Megatron-LM paper.\nThere are ways to reduce computational overhead by calculating the softmax required for the backward pass (we essentially have everything ready after communicating the log-sum-exp values) and even looking into fused cross-entropy8, but that is basically it.\n8 https://github.com/mgmalek/efficient_cross_entropy."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#communication-overhead",
    "href": "posts/numpitron_tensor_parallel/index.html#communication-overhead",
    "title": "Tensor Parallelism",
    "section": "Communication Overhead",
    "text": "Communication Overhead\nThe table below gives us the forward and backward pass communications required across devices for the Megatron-LM style tensor parallel transformer.\n\n\n\n\n\n\n\n\nLayer\nForward Pass\nBackward Pass\n\n\n\n\nInput Embedding\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\nN/A\n\n\nAttention\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\n\nMLP\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\n\nOutput Embedding\nN/A\n\\(N_\\text{batch} \\times N_\\text{ctx} \\times D_\\text{model}\\)\n\n\nCross-Entropy\n\\(3 \\cdot N_\\text{batch} \\times N_\\text{ctx}\\)\nN/A\n\n\n\nWe need to keep in mind that the Attention and MLP layers are most important here, since these will happen at every layer. It does seem to scale quite bad in terms of sequence length, something I might cover in an upcoming post when I add sequence parallel to NuMPItron.\n\nActivation Checkpointing\nHidden activations required for backward-pass calculations can quickly fill up device memory, and the larger the model size the faster memory fills up. To alleviate this somewhat, you can use activation checkpointing, which re-calculates the hidden activations during the backward pass by storing only the layer input instead.\nThis is a very useful technique to use, but when used naively it requires two additional ALLREDUCE operations when re-calculating the forward pass activations for the backward pass. The Megatron-LM authors fixed this in favor of a more selective recomputation in the sequence parallel paper9.\n9 See Reducing Activation Recomputation in Large Transformer Models."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#pipeline-and-data-parallel",
    "href": "posts/numpitron_tensor_parallel/index.html#pipeline-and-data-parallel",
    "title": "Tensor Parallelism",
    "section": "Pipeline and Data Parallel",
    "text": "Pipeline and Data Parallel\nPipeline parallelism splits the layers into stages, forwarding data between them. A common issue with this approach is the potential for ‘bubbles’ of inactivity. To mitigate this, data is sent in ‘microbatches.’ For instance, if the batch size is 8 and the microbatch size is 4, two microbatches of size 4 are sent. Increasing the number of microbatches reduces the bubble but generally also decreases the microbatch size.\n\n\n\nA very basic view of pipeline parallel. As we increase the number of microbatches, both the microbatch size and the bubble decreases. device 1 calculates the forward pass of one microbatch and forwards it to device 2, who calculates the forward pass and backwards pass before sending it back.\n\n\nThe number of pipeline stages determines the device spread, and the number of microbatches informs the global batch size10 needed for one forward and backward step. We want to know both! Unfortunately, Meta did not provide this information. We can, however, derive the data parallel value from reasonable default cases of pipeline parallel11.\n10 global batch size = number of microbatches * microbatch size * \\(N_\\text{DP}\\)11 To reduce the search space, we will set some good baseline values: the microbatch size to 1 (low bubble), the number of microbatches are a ‘reasonable’ multiple of the number of pipeline layers.\n\n\n\n\n\n\n\n\n\n\\(N_\\text{PP}\\)\n\\(N_\\text{DP}\\)\nnumber of microbatches\nGlobal batch size\nGlobal batch size \\(\\times\\) sequence length\n\n\n\n\n4\n512\n8\n4096\n33M\n\n\n4\n512\n12\n6144\n50M\n\n\n4\n512\n16\n8192\n67M\n\n\n8\n256\n12\n3072\n25M\n\n\n8\n256\n16\n4096\n33M\n\n\n8\n256\n20\n5120\n42M\n\n\n16\n128\n20\n2560\n21M\n\n\n16\n128\n24\n3072\n25M\n\n\n16\n128\n28\n3584\n30M\n\n\n\nAs we see, with an increase of \\(N_\\text{TP}\\) we decrease the global batch size potential. Some of these values look huge, but what can we compare it with:\n\nLLaMA-2-70B was trained on 2B global batch size in tokens\nDeepseek-67B was trained on 19.4M batch size in tokens\nOPT-175B was trained on 2M batch size in tokens\n\nAs an aside, pipeline parallelism is not required in combination with data and tensor parallel, OPT-175B was trained using only tensor-parallel and FSDP on 992 GPUs12.\n12 From the log-book, we have \\(N_\\text{TP}=8\\) on 992 GPUs, leaving 124 groups of 8 GPUs for data parallel training. We also have a local batch size of 8 for each group and a context length of 2048. Multiply 128 group by 8 local batch size by 2048 context length and we have 2031616, roughly 2M."
  },
  {
    "objectID": "posts/numpitron_tensor_parallel/index.html#a-visual-example",
    "href": "posts/numpitron_tensor_parallel/index.html#a-visual-example",
    "title": "Tensor Parallelism",
    "section": "A Visual Example",
    "text": "A Visual Example\nIt’s hard to visualize over 16K GPUs sharding a model, so let’s restrict ourselves to the following setup: \\(N_\\text{TP}=4\\), \\(N_\\text{PP} = 2\\) and \\(n_\\text{DP}=2\\). That is, four nodes with 4 GPUs each with two pipeline stages and 2 data parallel stages. Intra-node tensor parallelism is fast, we can have a large number of microbatches to ensure communication between nodes does not bother us and data parallel is essentially the cheapest parallelization techniques in terms of overhead:\n\n\n\n\\(N_\\text{TP}=4\\), \\(N_\\text{PP} = 2\\) and \\(n_\\text{DP}=2\\). A global batch gets sharded across devices in data parallel mode, the first pipeline layer has half of the transformer weights and uses tensor parallel for fast communication, after which it sends it the data to the second pipeline layer.\n\n\nThat wraps it up for this post. I hope this post helped you understand tensor parallel and perhaps even 3D parallel a bit better. Please let me know what you think, you can contact me on 𝕏. The next post will either discuss pipeline parallel in more depth or distributed sampling strategies, after implementing it in NuMPItron ofcourse."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hey, this is the website of Laurens Weitkamp. I’m a machine learning engineer interested in sequential decision making, computer vision, and multimodal models.\nI work for Thirona, where I train deep nets for medical imaging. Before that, I got my graduate degree in Artifical Intelligence where I specialized in hierarchical reinforcement learning and published a paper on explainable RL.\n\n\nRecent Posts\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPacking Data for efficient Training and Inference\n\n\nOct 1, 2024\n\n\n\n\nQLoRA Weight Dequantizing in Triton\n\n\nOct 1, 2024\n\n\n\n\nTensor Parallelism\n\n\nJun 14, 2024\n\n\n\n\nCollective Communications Reference\n\n\nJan 25, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/collective_communications/index.html",
    "href": "posts/collective_communications/index.html",
    "title": "Collective Communications Reference",
    "section": "",
    "text": "This is a reference page I made of collective communications in multiprocessing systems while working on NuMPItron. I previously made a blog post about tensor parallel training in NuMPItron here. This article is in some sense a work in progress and I will keep updating it as I read more papers.\nThere is a general guide to performant versions of each comm here, although it is very specific for MPI and the same does not have to hold for GPUs. Gloo, created by Meta, also has a decent reference guide."
  },
  {
    "objectID": "posts/collective_communications/index.html#ring-allreduce",
    "href": "posts/collective_communications/index.html#ring-allreduce",
    "title": "Collective Communications Reference",
    "section": "2.1 Ring AllReduce",
    "text": "2.1 Ring AllReduce\nHorovod, ye old distributed training framework for TensorFlow built by Uber back in the day, was too slow. Specifically, the AllReduce implementation was too slow. Originally, the algorithm implementation performed something like a Reduce - Section 7 and Broadcast - Section 4 operation on the data. A single/root device would collect the data, perform the reduction, and send it to all other devices. The issue is that it creates a bandwidth bottleneck: increase the number of devices and the incoming root will not be able to receive and send data fast enough.\nAn engineer working at Baidu Research figured out an improvement based on a technique in high performance computing, a ring message passing scheme (the same one described in the paper above). If you picture the devices in a topological ring, each device receives data from its left neighbour and sends data to its right neighbour. In the figure below1 we have three devices and three chunks of data. The improved algorithm starts with a series of ReduceScatter - Section 8 operations where each device sends one of its chunk to its neighbor untill each device has one complete chunk:\n1 I’ve adapted and simplified the figure below from the original blog post (andrew.gibiansky.com).\nWith each device having a complete chunk of the data, each device now sends its neighbor the complete chunk it has untill each device has the data in full:\n\nThis implementation makes scaling distributed data parallel much more efficient, and was probably somewhat of a milestone in distributed training frameworks."
  },
  {
    "objectID": "posts/packing/index.html",
    "href": "posts/packing/index.html",
    "title": "Packing Data for efficient Training and Inference",
    "section": "",
    "text": "The context length of transformer models is rapidly increasing with each new generation of frontier models. For instance, Google’s latest Gemini 1.5 model can process up to 1 million tokens, equivalent to 1 hour of video or approximately 700,000 words. This increase is driven by advancements in hardware and the development of more efficient algorithms tailored to the specific demands of long-context training1.\nLong contexts introduce several challenges: identifying data that inherently requires long contexts (such as books, podcasts, and videos) and efficiently handling smaller context data. Simply concatenating these documents and feeding them into an autoregressive transformer can lead to cross-contamination of sequences. For instance, predicting a sequence S2 from an unrelated sequence S1 becomes nearly impossible if there is a significant difference in their subject matter.\nThe same issue arises during inference. If we dynamically batch tokens from different users, efficiency can be improved by grouping shorter conversations in the same batch. The solution to both of these issues is a technique called packing."
  },
  {
    "objectID": "posts/packing/index.html#literature-review",
    "href": "posts/packing/index.html#literature-review",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Literature Review",
    "text": "Literature Review\nPacking is briefly described in most papers on language modelling (in fact, most authors cite T5 for it), here are some from the literature:\n\n\n\nPaper\nQuote\n\n\n\n\nRoBERTa2\n“Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens.”\n\n\nGPT-33\n“During training we always train on sequences of the full nctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency.”\n\n\nT54\n“Whenever possible, we “pack” multiple sequences into each entry of the batch so that our batches contain roughly 216 = 65,536 tokens.”\n\n\nT05\n“we use packing to combine multiple training examples into a single sequence to reach the maximum sequence length.”\n\n\n\n2 RoBERTa: A Robustly Optimized BERT Pretraining Approach3 Language Models are Few-Shot Learners4 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer5 Efficient Sequence Packing without Cross-contamination6 Efficient Sequence Packing without Cross-contaminationSome approaches use the packing described above naively and add an end-of-document token after the sequence is done to let the model figure out the difference between samples. This does seem to hurt performance due to cross-contamination mentioned before, but it might not have as big of an impact at scale6."
  },
  {
    "objectID": "posts/packing/index.html#dealing-with-cross-contamination",
    "href": "posts/packing/index.html#dealing-with-cross-contamination",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Dealing with Cross Contamination",
    "text": "Dealing with Cross Contamination\nTo prevent cross-contamination when packing sequences, it is crucial to update both the positional information and the attention masks, not just concatenating the sequences. This ensures that the attention mechanism is still valid.\n\nUpdate Positional Information\nWhen using absolute positional encodings (or ALiBi, or absolute encodings), we need to reset the positional encoding index at the start of each new sample within the packed sequence. This reset ensures that each sequence retains its positional context, which is essential for the model’s accuracy.\n\n\n\nPacked positional encodings.\n\n\nFor RoPE, you would have to do a similar rest.\n\n\nUpdate the Attention Mask\nIn self-attention mechanisms, it is necessary to ensure that one sequence cannot attend to another within the same packed context. This requires merging autoregressive masks for the self-attention layer, enforcing the restriction effectively:\n\n\n\nA packed attention mask.\n\n\nThis might be the most intrusive part to the codebase - it will require actually calculating a distinct attention mask per batch where we otherwise can simply have the standard diagonal created once for the whole batch. This is probably the deal-breaker for most codebases, since you need to implement it effectively otherwise flash attention will not work, you increase mask memory consumption, etc.\n\n\nThat’s it!\nProperly implementing these updates ensures that the model’s gradient step remains consistent, regardless of whether sequences are processed individually or packed.\nIt’s worth your time looking at figures 3 and 4, it’s from a paper that introduces the concept of cross-contamination. The paper additionally explains the performance of packing and the effect of ‘proper’ masking, it’s a great read!\nAnother paper that discusses masked packing but during inference time that goes in depth on the effect of packing on prefilling and the time to first token is Prepacking."
  },
  {
    "objectID": "posts/packing/index.html#can-we-ignore-cross-contamination",
    "href": "posts/packing/index.html#can-we-ignore-cross-contamination",
    "title": "Packing Data for efficient Training and Inference",
    "section": "Can we Ignore Cross Contamination?",
    "text": "Can we Ignore Cross Contamination?\nFrom personal experience, there is little support for packing in public codebases, often justified by the assumption that the data is not too correlated. That could be true, but it is a strong assumption to make. Interestingly, the approach in Structured Packing in LLM Training Improves Long Context Utilization essentially ignores proper masking to benefit long-context learning. However, the models are small scale and the data is not inherently long context.\nGiven that t5x does have a proper packing implementation7, I would bet that Google did some internal checks to see if it helps or not. You can choose to ignore it, but you probably don’t want to risk it at the frontier level where your infra is optimized anyway.\n\n\n7 They call it segmented data - from the T5x codebase."
  }
]